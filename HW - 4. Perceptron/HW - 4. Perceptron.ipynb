{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea48a446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "076da5ba",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><center><h1>Биологический нейрон</h1></center></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2255b724",
   "metadata": {},
   "source": [
    "**Нейрон** — электрически возбудимая клетка, которая предназначена для приёма извне, обработки, хранения, передачи и вывода вовне информации с помощью электрических и химических сигналов.  \n",
    "  \n",
    "Типичный нейрон состоит из тела клетки, дендритов и одного аксона:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa916480",
   "metadata": {},
   "source": [
    "![neuron](images/neuron.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30244433",
   "metadata": {},
   "source": [
    "Простыми словами можно описать принцип действия нейрона следующим образом:\n",
    "- через дендриты в нейрон поступают сигналы (раздражители)\n",
    "- Если комбинация сигналов превышает пороговый уровень - нейрон \"выстреливает\", т.е. передаёт сигнал дальше через аксон."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a7ce203",
   "metadata": {},
   "source": [
    "Нейроны могут соединяться один с другим, формируя нервные сети."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5200e275",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><center><h1>Функции Активации (Activation Functions)</h1></center></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d41f55f",
   "metadata": {},
   "source": [
    "Простейшим механизмом активации является **Step activation**, когда перспептрон передаёт на выход значение только в том случае, если сумма взвешенных входящих сигналов больше заданного порога:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff07a8e1",
   "metadata": {},
   "source": [
    "![step](images/step_activation.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3b588aa",
   "metadata": {},
   "source": [
    "При всей своей простоте, данная функция активации обладает критическим недостатком: она недифференцируемая.  Как результат, она не позволяет осуществлять процесс обучения персептрона.  \n",
    "  \n",
    "Для того, чтобы исправить это, было разработано множество других функций активаций, таких как:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "baf346ed",
   "metadata": {},
   "source": [
    "![neuron](images/activation_functions.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40d1d1b0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><center><h1>Задание 1 (2 балла)</h1></center></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6be49dd4",
   "metadata": {},
   "source": [
    "Напишите класс **ActivationFunction** и его подкласс **Sigmoid**, у которого будет функция `forward`, которая:\n",
    "- будет принимать на вход число и будет сохранять его внутри объекта\n",
    "- будет возвращать результат в соответствии с фукцией $\\sigma(x) = \\frac{1}{1 + e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a68d43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction:\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "\n",
    "    def forward(self, value: np.ndarray) -> np.ndarray:\n",
    "        self.input = value\n",
    "        self.output = 1.0/(1.0+np.exp(-self.input))\n",
    "        return self.output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5f342c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ceab0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68842773, 0.73079903])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid.forward(np.array([0.79277902, 0.99868032]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c54ea44",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78315b25",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53618f50",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><center><h1>Персептрон</h1></center></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "266324a8",
   "metadata": {},
   "source": [
    "**Персептрон** -  математическая модель биологического нейрона, является базовым элементом нейронных сетей:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c63f671b",
   "metadata": {},
   "source": [
    "![neuron](images/perceptron.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54cc65e4",
   "metadata": {},
   "source": [
    "**Персептрон** состоит из следующих ключевых элементов:\n",
    "- `вход` - отвечает за получение входных значений. Является аналогом дендрита биологического нейрона\n",
    "- `веса` - механизм \"важности\" входных значений. По аналогии с нейроном - это \"толщина\" дендрита\n",
    "- `функция активации` - обрабатывает сумму взвешенных входных сигналов и передаёт результат на выход\n",
    "- `выход` - отвечает за передачу итогового результата. Аналогичен аксону\n",
    "  \n",
    "Практически всегда к входным сигналам также добавляется \"bias\", который всегда = 1.  \n",
    "Это позволяет не привязывать выход персептрона к 0 в случае, если все входные сигналы также равны 0 (как в механизме регрессии)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "057194ee",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><center><h1>Задание 2 (4 балла)</h1></center></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d42423ee",
   "metadata": {},
   "source": [
    "напишите класс **Layer**, у когорого будут следующие входные параметры:\n",
    "- **n_inputs** - количество входящих значений\n",
    "- **n_outputs** - количество исходящих значений (в нашем случае = 1)\n",
    "- **activation** - объект из семейства **ActivationFunction** (в нашем случае - **Sigmoid**)\n",
    "  \n",
    "При своём создании объект класса **Layer** должен также создавать атрибут `weights_`, в ктором будут рандомально инициализированны веса для входящих значений, а также для `bias`\n",
    "\n",
    "Класс **Layer** должен иметь функцию `forward`, принимающую на вход массив *numpy*, и возвращающую результат функции активации (тоже в виде массива).  \n",
    "Также эта функция должна сохранять полученные на вход значения внутри экземпляра"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04ec3681",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self, n_inputs=np.array([1]), n_outputs=np.array([1]), activation=Sigmoid()):\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.input = None\n",
    "        self.z = None\n",
    "        self.output = None\n",
    "        self.activation = activation\n",
    "        self.bias = np.random.normal()\n",
    "        self.weights_ = np.random.normal(size=self.n_inputs)\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        self.input = X\n",
    "        self.z = []\n",
    "\n",
    "        for example in self.input:\n",
    "            self.z.append(example.dot(self.weights_.T) + self.bias)\n",
    "        \n",
    "        self.z = np.array(self.z)\n",
    "\n",
    "        self.output = self.activation.forward(self.z)\n",
    "\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84a33bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = Layer(np.array([5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16759700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96978013, 0.99999941])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.forward(np.array([[1,2,3,4,5],\n",
    "                        [6,7,8,9,10],]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2100ce8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ecfecac",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><center><h1>Задание 3 (2 балла)</h1></center></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15bcc0dc",
   "metadata": {},
   "source": [
    "напишите класс **LossFunction** и его подкласс **CrossEntropy**, у которого будет функция `loss`, которая будет принимать реальное бинарное значение *y_fact* и вероятность *y_prob* (оба параметра в виде np.array) и будет возвращать результат по формуле:  \n",
    "  \n",
    "$$\n",
    "L = - \\sum (y_{fact} * log(y_{prob}) + (1-y_{fact})*log(1-y_{prob}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40a91107",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction:\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self.y_fact = None\n",
    "        self.y_prob = None\n",
    "        self.error = None\n",
    "\n",
    "class CrossEntropy(LossFunction):\n",
    "\n",
    "    def loss(self, y_fact: np.ndarray, y_prob: np.ndarray) -> np.ndarray:\n",
    "        \n",
    "        self.y_fact = y_fact\n",
    "        self.y_prob = y_prob\n",
    "        self.error = np.array(-np.mean(self.y_fact * np.log(self.y_prob) + \\\n",
    "                        (1 - self.y_fact) * np.log(1 - self.y_prob)))\n",
    "\n",
    "        return self.error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "248e188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = CrossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16949e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.73987037)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy.loss(np.array([[0, 1]]), np.array([[0.68842773, 0.73079903]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0724135c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3adeb1c5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><center><h1>Обучение. Forward and Backpropagation</h1></center></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5305fa49",
   "metadata": {},
   "source": [
    "Процесс обучения персептрона (и в целом нейросети) итеративен и состоит из следующих этапов:\n",
    "- Сперва персептрон инициализируется с рандомальными весами\n",
    "- Осуществляется цикл \"вперёд\":\n",
    "  - Входные значения перемножаются с соответствующими весами и суммируются\n",
    "  - Эта сумма подаётся на функцию активации\n",
    "  - Функция активации возвращает итоговое значение\n",
    "- Итоговое значение сравнивается с ожидаемым и высчитывается ошибка (Loss)\n",
    "- Осуществляется цикл \"назад\":\n",
    "  - при помощи `Chain Rule` рассчитываются частичные производные для всех элементов персептрона\n",
    "  - исходя из заданного коэффициента обучения (`learning rate`, $\\alpha$), веса $w_{i}$ корректируются\n",
    "- Данный цикл повторяется заданное количество раз или до тех пор, пока итоговая ошибка не опустится ниже заданного порогового значения"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "308f269a",
   "metadata": {},
   "source": [
    "![img](images/training.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "561a0181",
   "metadata": {},
   "source": [
    "### <center>Chain Rule</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "920cc590",
   "metadata": {},
   "source": [
    "Если нам дана функция $y=f(u)$, где $u = g(x)$, то тогда производная этой функции по $x$ будет равно:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48de5834",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dy}{dx} = \\frac{dy}{du}\\frac{du}{dx}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0f31ddb",
   "metadata": {},
   "source": [
    "Тогда для того, чтобы понять, насколько изменение весов $w$ влияет на изменение $y$ (т.е. производные $\\frac{dy}{dw_{i}}$), можно вычислить следующие производные:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d239444",
   "metadata": {},
   "source": [
    "![neuron](images/backpropagation.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6568f1db",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><center><h1>Задание 4 (8 баллов)</h1></center></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7be53a81",
   "metadata": {},
   "source": [
    "Модифицируйте классы **Layer**, **LossFuncton** и **ActivationFunction** таким образом, чтобы можно было рассчитать их частичные производные, и добавьте функцию `back`, позволяющую осуществить backpropagation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9534b596",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><center><h3>Это задание очень сложное, и даже частичное его выполнение будет учитываться</h3></center></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42c62fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy(LossFunction):\n",
    "\n",
    "    def loss(self, y_fact: np.ndarray, y_prob: np.ndarray) -> np.ndarray:\n",
    "        \n",
    "        self.y_fact = y_fact\n",
    "        self.y_prob = y_prob\n",
    "        self.error = np.array(-np.mean(self.y_fact * np.log(self.y_prob) + \\\n",
    "                        (1 - self.y_fact) * np.log(1 - self.y_prob)))\n",
    "        return self.error\n",
    "\n",
    "    def prime(self) -> float:\n",
    "        dLda = np.mean(((self.y_fact - 1)/(self.y_prob - 1) - self.y_fact/self.y_prob))\n",
    "        return dLda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e165345",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(ActivationFunction):\n",
    "\n",
    "    def forward(self, value: np.ndarray) -> np.ndarray:\n",
    "        self.input = value\n",
    "        self.output = 1.0/(1.0+np.exp(-self.input))\n",
    "        return self.output\n",
    "\n",
    "    def prime(self) -> float:\n",
    "        dadz = np.mean(self.output * (1 - self.output))\n",
    "        return dadz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b6c1df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self, n_inputs=np.array([1]), n_outputs=np.array([1]), activation=Sigmoid()):\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.input = None\n",
    "        self.z = None\n",
    "        self.output = None\n",
    "        self.activation = activation\n",
    "        self.bias = np.random.normal()\n",
    "        self.weights_ = np.random.normal(size=self.n_inputs)\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        self.input = X\n",
    "        self.z = []\n",
    "\n",
    "        for example in self.input:\n",
    "            self.z.append(example.dot(self.weights_.T) + self.bias)\n",
    "        \n",
    "        self.z = np.array(self.z)\n",
    "\n",
    "        self.output = self.activation.forward(self.z)\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def update_weights(self, learning_rate, dLda, dadz) -> None:\n",
    "\n",
    "        dzdw = np.mean(self.input, axis=0)\n",
    "        dzdb = 1\n",
    "\n",
    "        self.weights_ -= learning_rate * dLda * dadz * dzdw\n",
    "        self.bias -= learning_rate * dLda * dadz * dzdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98e79191",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron():\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_inputs=np.array([1]),\n",
    "                 activation=Sigmoid(),\n",
    "                 lossfucn=CrossEntropy(),\n",
    "                 lerning_rate=0.05\n",
    "                 ) -> None:\n",
    "        \n",
    "        self.layer = Layer(n_inputs=n_inputs, activation=activation)\n",
    "        self.lossfucn = lossfucn\n",
    "        self.output = None\n",
    "        self.learning_rate = lerning_rate\n",
    "    \n",
    "    def forward(self, X:np.ndarray) -> np.ndarray:\n",
    "        self.input = X\n",
    "        self.output = self.layer.forward(self.input)\n",
    "        return self.output\n",
    "    \n",
    "    def loss_by_batch(self, y:np.ndarray) -> float:\n",
    "        self.loss = self.lossfucn.loss(y, self.output)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, y:np.ndarray) -> None:\n",
    "        self.loss = self.loss_by_batch(y)\n",
    "        self.dLda = self.lossfucn.prime()\n",
    "        self.dadz = self.layer.activation.prime()\n",
    "        self.layer.update_weights(self.learning_rate, self.dLda, self.dadz)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9acf4160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  y\n",
       "0   1   1  1\n",
       "1   1   0  0\n",
       "2   0   1  0\n",
       "3   0   0  0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Логческое И\n",
    "df = pd.DataFrame(data={'x1': [1,1,0,0],\n",
    "                        'x2': [1,0,1,0],\n",
    "                        'y': [1,0,0,0],})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7ed4b0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start weights: [0.41788222 0.18226602]\n",
      "start bias: -0.4425563783241034\n",
      "Epoch: 0, Loss: 0.6665034100784957\n",
      "Epoch: 100, Loss: 0.614819595386098\n",
      "Epoch: 200, Loss: 0.5896827875779689\n",
      "Epoch: 300, Loss: 0.5770719976728544\n",
      "Epoch: 400, Loss: 0.5705626080262168\n",
      "Epoch: 500, Loss: 0.5671236861771021\n",
      "Epoch: 600, Loss: 0.5652737465577384\n",
      "Epoch: 700, Loss: 0.564264710825426\n",
      "Epoch: 800, Loss: 0.563708488466264\n",
      "Epoch: 900, Loss: 0.5633993727433512\n",
      "Epoch: 1000, Loss: 0.5632264907108242\n",
      "Epoch: 1100, Loss: 0.5631293084405891\n",
      "Epoch: 1200, Loss: 0.563074447139076\n",
      "Epoch: 1300, Loss: 0.56304336079882\n",
      "Epoch: 1400, Loss: 0.563025683800773\n",
      "Epoch: 1500, Loss: 0.5630155956609101\n",
      "Epoch: 1600, Loss: 0.563009815777907\n",
      "Epoch: 1700, Loss: 0.5630064892633038\n",
      "Epoch: 1800, Loss: 0.5630045643993835\n",
      "Epoch: 1900, Loss: 0.5630034432711983\n",
      "Epoch: 2000, Loss: 0.5630027850245423\n",
      "Epoch: 2100, Loss: 0.5630023947625584\n",
      "Epoch: 2200, Loss: 0.563002160655449\n",
      "Epoch: 2300, Loss: 0.5630020182661635\n",
      "Epoch: 2400, Loss: 0.5630019302733944\n",
      "Epoch: 2500, Loss: 0.5630018749222672\n",
      "Epoch: 2600, Loss: 0.5630018394309657\n",
      "Epoch: 2700, Loss: 0.5630018162165156\n",
      "Epoch: 2800, Loss: 0.5630018007272737\n",
      "Epoch: 2900, Loss: 0.5630017901931016\n",
      "Epoch: 3000, Loss: 0.5630017829010399\n",
      "Epoch: 3100, Loss: 0.5630017777728669\n",
      "Epoch: 3200, Loss: 0.5630017741167703\n",
      "Epoch: 3300, Loss: 0.5630017714799525\n",
      "Epoch: 3400, Loss: 0.5630017695601084\n",
      "Epoch: 3500, Loss: 0.563001768151524\n",
      "Epoch: 3600, Loss: 0.5630017671117243\n",
      "Epoch: 3700, Loss: 0.5630017663404681\n",
      "Epoch: 3800, Loss: 0.563001765766262\n",
      "Epoch: 3900, Loss: 0.5630017653375274\n",
      "Epoch: 4000, Loss: 0.5630017650167012\n",
      "Epoch: 4100, Loss: 0.5630017647762176\n",
      "Epoch: 4200, Loss: 0.5630017645957244\n",
      "Epoch: 4300, Loss: 0.563001764460124\n",
      "Epoch: 4400, Loss: 0.5630017643581748\n",
      "Epoch: 4500, Loss: 0.5630017642814829\n",
      "Epoch: 4600, Loss: 0.5630017642237662\n",
      "Epoch: 4700, Loss: 0.5630017641803162\n",
      "Epoch: 4800, Loss: 0.5630017641475982\n",
      "Epoch: 4900, Loss: 0.563001764122957\n",
      "Epoch: 5000, Loss: 0.5630017641043962\n",
      "Epoch: 5100, Loss: 0.5630017640904139\n",
      "Epoch: 5200, Loss: 0.56300176407988\n",
      "Epoch: 5300, Loss: 0.5630017640719435\n",
      "Epoch: 5400, Loss: 0.5630017640659636\n",
      "Epoch: 5500, Loss: 0.5630017640614579\n",
      "Epoch: 5600, Loss: 0.5630017640580629\n",
      "Epoch: 5700, Loss: 0.5630017640555045\n",
      "Epoch: 5800, Loss: 0.5630017640535767\n",
      "Epoch: 5900, Loss: 0.5630017640521242\n",
      "Epoch: 6000, Loss: 0.5630017640510295\n",
      "Epoch: 6100, Loss: 0.5630017640502047\n",
      "Epoch: 6200, Loss: 0.5630017640495832\n",
      "Epoch: 6300, Loss: 0.5630017640491148\n",
      "Epoch: 6400, Loss: 0.5630017640487619\n",
      "Epoch: 6500, Loss: 0.5630017640484959\n",
      "Epoch: 6600, Loss: 0.5630017640482956\n",
      "Epoch: 6700, Loss: 0.5630017640481444\n",
      "Epoch: 6800, Loss: 0.5630017640480307\n",
      "Epoch: 6900, Loss: 0.563001764047945\n",
      "Epoch: 7000, Loss: 0.5630017640478803\n",
      "Epoch: 7100, Loss: 0.5630017640478315\n",
      "Epoch: 7200, Loss: 0.5630017640477949\n",
      "Epoch: 7300, Loss: 0.5630017640477674\n",
      "Epoch: 7400, Loss: 0.5630017640477464\n",
      "Epoch: 7500, Loss: 0.5630017640477307\n",
      "Epoch: 7600, Loss: 0.5630017640477188\n",
      "Epoch: 7700, Loss: 0.56300176404771\n",
      "Epoch: 7800, Loss: 0.5630017640477032\n",
      "Epoch: 7900, Loss: 0.5630017640476981\n",
      "Epoch: 8000, Loss: 0.5630017640476943\n",
      "Epoch: 8100, Loss: 0.5630017640476914\n",
      "Epoch: 8200, Loss: 0.5630017640476893\n",
      "Epoch: 8300, Loss: 0.5630017640476876\n",
      "Epoch: 8400, Loss: 0.5630017640476864\n",
      "Epoch: 8500, Loss: 0.5630017640476855\n",
      "Epoch: 8600, Loss: 0.5630017640476848\n",
      "Epoch: 8700, Loss: 0.5630017640476843\n",
      "Epoch: 8800, Loss: 0.5630017640476839\n",
      "Epoch: 8900, Loss: 0.5630017640476837\n",
      "Epoch: 9000, Loss: 0.5630017640476834\n",
      "Epoch: 9100, Loss: 0.5630017640476832\n",
      "Epoch: 9200, Loss: 0.563001764047683\n",
      "Epoch: 9300, Loss: 0.563001764047683\n",
      "Epoch: 9400, Loss: 0.5630017640476829\n",
      "Epoch: 9500, Loss: 0.5630017640476829\n",
      "Epoch: 9600, Loss: 0.5630017640476828\n",
      "Epoch: 9700, Loss: 0.5630017640476828\n",
      "Epoch: 9800, Loss: 0.5630017640476828\n",
      "Epoch: 9900, Loss: 0.5630017640476828\n",
      "end predict [0.24696121 0.27308387 0.22887961 0.25373411]\n",
      "end loss: 0.5630017640476828\n",
      "end weights: [ 0.09976289 -0.13585331]\n",
      "end bias: -1.078795037395873\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron(np.array([2]), lerning_rate = 0.01)\n",
    "\n",
    "print(f'start weights: {perceptron.layer.weights_}')\n",
    "print(f'start bias: {perceptron.layer.bias}')\n",
    "\n",
    "for epoch in range(10000):\n",
    "\n",
    "    # dff = df.sample(np.random.randint(1, 5), replace=True)\n",
    "    # dff = df.sample(10, replace=True)\n",
    "    # dff = df\n",
    "    dff = df.iloc[0:4, :]\n",
    "    X = dff.iloc[:, :-1].to_numpy()\n",
    "    y = dff.iloc[:, -1:].to_numpy()\n",
    "\n",
    "    perceptron.forward(X)\n",
    "    perceptron.backward(y)\n",
    "\n",
    "    loss = perceptron.loss\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss}\")\n",
    "\n",
    "X = df.iloc[:, :-1].to_numpy()\n",
    "y = df.iloc[:, -1:].to_numpy()\n",
    "\n",
    "print(f'end predict {perceptron.forward(X)}')\n",
    "perceptron.loss_by_batch(y)\n",
    "print(f'end loss: {perceptron.loss}')\n",
    "print(f'end weights: {perceptron.layer.weights_}')\n",
    "print(f'end bias: {perceptron.layer.bias}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e5a244e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90f00233",
   "metadata": {},
   "source": [
    "# <center>Удачи!</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
